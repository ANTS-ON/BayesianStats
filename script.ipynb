{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c33d62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "  <img style=\"padding-left: 50px; height: 600px; float: left;\" src=\"data/frequentists_vs_bayesians_1.png\"/>\n",
    "  <img style=\"padding-top: 75px; padding-left: 50px; height: 550px; float: left;\" src=\"data/frequentists_vs_bayesians_2.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab83fc8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac91079",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Statistics from Scratch\n",
    "\n",
    "Anton Stratmann, 2nd year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845bdb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Roadmap\n",
    "\n",
    "1. Understanding Bayes' theorem\n",
    "2. Priors\n",
    "3. Towards application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bbc57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assumptions matter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f776a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The frequetist calculates $\\mathbb{P}\\left(\\, \\text{\"Positive\"} \\, \\big| \\, \\text{\"Sun Exploded\"} \\, \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9131af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, we are interested in $\\mathbb{P}\\left(\\, \\text{\"Sun Exploded\"} \\, | \\, \\text{\"Positive\"} \\, \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7760acb3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sun explosion is an unlikely event &rarr; Account for that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b60519",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a49201c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mm(graph):\n",
    "  graphbytes = graph.encode(\"ascii\")\n",
    "  base64_bytes = base64.b64encode(graphbytes)\n",
    "  base64_string = base64_bytes.decode(\"ascii\")\n",
    "  display(\n",
    "    Image(\n",
    "      url=\"https://mermaid.ink/img/\"\n",
    "      + base64_string,\n",
    "      width=600\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8638f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mm(\"\"\"graph LR; A(( )) --A--> B(Sun Exploded); A --1-A--> C(Sun Not Exploded); B --35/36--> D(Positive); B --1/36--> E(Negative); C --1/36--> F(Positive); C --35/36--> G(Negative)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f977a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deriving Bayes' theorem\n",
    "\n",
    "- How can we get $\\mathbb{P}\\left(\\,\\text{\"Sun Exploded\"} \\, \\big| \\, \\text{\"Positive\"} \\, \\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c749e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use laws of conditional probability: $\\mathbb{P}\\left(\\, B \\, \\big| \\, A \\, \\right) = \\frac{\\mathbb{P}\\left(\\, B \\, \\bigcap \\, A \\, \\right)}{\\mathbb{P}\\left(\\, A \\, \\right)}$\n",
    "- And by replacing\n",
    "    - $\\mathbb{P}\\left(\\, B \\, \\bigcap \\, A \\, \\right) = \\mathbb{P}\\left(\\, A \\, \\big| \\, B \\, \\right) \\cdot \\mathbb{P}\\left(\\, B \\, \\right)$\n",
    "    - $\\mathbb{P}\\left(\\, A \\, \\right) = \\mathbb{P}\\left(\\, A \\, \\big| \\, B \\, \\right) \\cdot \\mathbb{P}\\left(\\, B \\, \\right) + \\mathbb{P}\\left(\\, A \\, \\big| \\, B ^ \\complement \\, \\right) \\cdot \\mathbb{P}\\left(\\, B ^ \\complement \\, \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a4995",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Bayes' theorem: $$\\mathbb{P}\\left(\\, B \\, \\big| \\, A \\, \\right) = \\frac{\\mathbb{P}\\left(\\, A \\, \\big| \\, B \\, \\right) \\cdot \\mathbb{P}\\left(\\, B \\,\\right)}{\\mathbb{P}\\left(\\, A \\, \\big| \\, B \\, \\right) \\cdot \\mathbb{P}\\left(\\, B \\, \\right) + \\mathbb{P}\\left(\\, A \\, \\big| \\, B ^ \\complement \\, \\right) \\cdot \\mathbb{P}\\left(\\, B ^ \\complement \\, \\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1639f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes' theorem for densities\n",
    "\n",
    "- Sets have limited application &rarr; We need a more general Bayes' theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64816309",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bayes' theorem also holds for probability densities\n",
    "- It can be applied easily to parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4404a47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"padding: 10px; padding-left: 50px; background: #D3D3D3;\">\n",
    "    Given some data $\\mathbf{y}$ and parameters $\\boldsymbol\\theta$ that have a joint probability density $p\\big(\\mathbf{y}, \\boldsymbol\\theta\\big)$. Then, we obtain\n",
    "    $$p\\left(\\boldsymbol\\theta \\, \\big| \\, \\mathbf{y}\\right) = \\frac{p\\left(\\mathbf{y} \\, \\big| \\, \\boldsymbol\\theta\\right) \\cdot p\\left(\\boldsymbol\\theta\\right)}{p\\left(\\mathbf{y}\\right)}$$\n",
    "    also called Bayes' theorem. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b699e5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p\\left(\\boldsymbol\\theta\\right)$ is called *prior*, $p\\left(\\mathbf{y} \\, \\big| \\, \\boldsymbol\\theta\\right)$ *likelihood* and $p\\left(\\boldsymbol\\theta \\, \\big| \\, \\mathbf{y}\\right)$ *posterior*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689aa95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implications\n",
    "\n",
    "- In Bayesian statistics, parameters are random\n",
    "- Their distribution is updated from *prior* to *posterior* density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1f348",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The *prior* encodes assumptions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9bb2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior normalization is the bottleneck\n",
    "- *likelihood* and *prior* are usually known &rarr; *posterior* is known up to a constant factor\n",
    "- Normalization of the posterior is often hard (high-dimensional integral)\n",
    "- Bayesian inference often circumvents normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ee517",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements for Bayesian inference\n",
    "- *likelihood* and *prior* densities need not be normalized\n",
    "- Work with log densities: $\\log p\\left(\\mathbf{y} \\, \\big| \\, \\boldsymbol\\theta\\right) \\cdot p\\left(\\boldsymbol\\theta\\right) = \\log p\\left(\\mathbf{y} \\, \\big| \\, \\boldsymbol\\theta\\right) + \\log p\\left(\\boldsymbol\\theta\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f3911",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bayesian inference usually requires $\\log p\\left(\\mathbf{y} \\, \\big| \\, \\boldsymbol\\theta\\right)$ and $\\log p\\left(\\boldsymbol\\theta\\right)$ up to a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88428193",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentism\n",
    "\n",
    "- Frequentism is a statistical paradigm present since the early 20th century\n",
    "- Taught in the average statistics lecture\n",
    "- Universal tools: maximum likelihood theory, hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda0c85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Aim**: eliminate subjective view, objective statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf7fe4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key concept**: Uncertainty arises because we observe limited (subsets of) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853bf54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarities\n",
    "\n",
    "- Both have common mathematical base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f705dff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Maximum likelihood theory is closely linked to Bayesian inference\n",
    "    - Likelihood function used for MLE &lrarr; Bayesian likelihood density\n",
    "    - Asymptotic normality has implications for both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6b9d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both paradigms can yield similar results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213c707",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166d316",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Frequentist statistics does not incorporate assumptions (officially ;))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028dd01b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Bayesian statistics does not have ground truth parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edd6a00",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Bayesian statistics generally require more computational resources, but is much more versatile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bcd4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Priors\n",
    "\n",
    "We discuss, how\n",
    "- priors encode assumptions\n",
    "- priors can be interpreted as regularizers\n",
    "- to choose a prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec61f6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Priors encode assumptions\n",
    "- Prior elucidation typically considers previous beliefs or knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd8cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Beliefs can be subjective and should have some objective reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a71c79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Knowledge are objective, consensus-based assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f84ec3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Priors can also be given by previous experiments (Bayesian updating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290a7e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization is often Maximum Likelihood\n",
    "- Example: Weighted Least Squares (WLS) \n",
    "    $$\\big|\\big|\\mathbf{\\Sigma}^{-\\frac{1}{2}}\\big(F(\\boldsymbol \\theta) - \\mathbf{y}\\big)\\big|\\big|^2_2 = \\big(F(\\boldsymbol \\theta) - \\mathbf{y}\\big)^\\top \\mathbf{\\Sigma}^{-1} \\big(F(\\boldsymbol \\theta) - \\mathbf{y}\\big) = \\sum_{i = 0}^N \\left( \\frac{F_i(\\boldsymbol \\theta) - y_i}{\\sigma_i} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cf477",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can now see that\n",
    "    $$\\frac{1}{\\sqrt{(2 \\pi)^n |\\mathbf{\\Sigma}|}} \\mathrm{exp}\\left(-\\frac{1}{2}\\big(F(\\boldsymbol \\theta) - \\mathbf{y}\\big)^\\top \\mathbf{\\Sigma}^{-1} \\big(F(\\boldsymbol \\theta) - \\mathbf{y}\\big)\\right)$$\n",
    "  is a multivariate Normal density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20348904",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Minimum of WLS is an MLE assuming Normal-distributed data $\\mathbf{y} \\sim \\mathcal{N}\\big(F(\\boldsymbol \\theta), \\mathbf{\\Sigma}\\big)$\n",
    "- Can also be used as Bayesian *likelihood*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f995989",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems are often ill-conditioned\n",
    "- Example: Standard linear least squares ($ \\min_{\\boldsymbol \\theta} ||\\mathbf{A} \\cdot \\boldsymbol \\theta - \\mathbf{y}||^2_2 $) with matrix\n",
    "$$ \\mathbf{A} = \n",
    "    \\begin{pmatrix} \n",
    "        -2 & 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ \n",
    "         1 & -2 & 1 & 0 & \\cdots & \\cdots & 0 \\\\\n",
    "         0 & 1 & -2 & 1 & 0 & \\dots & 0 \\\\ \n",
    "         \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n",
    "         0 & \\cdots & 0 & 1 & -2 & 1 & 0 \\\\ \n",
    "         0 & \\cdots & \\cdots & 0 & 1 & -2 & 1 \\\\ \n",
    "         0 & \\cdots & \\cdots &\\cdots & 0 & 1 & -2 \n",
    "    \\end{pmatrix}$$\n",
    "- Bad condition: Errors in input data are amplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eebb53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution: Regularization\n",
    "- Idea: trade approximation quality for less error amplification\n",
    "- Tikhonov/L2 regularization: solve instead\n",
    "    $$\\min_{\\boldsymbol \\theta} ||\\mathbf{A} \\cdot \\boldsymbol \\theta - \\mathbf{y}||^2_2 + \\alpha \\cdot ||\\boldsymbol \\theta||^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45479b53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This looks like a prior!\n",
    "- Closely look at\n",
    "    $$\\min_{\\boldsymbol \\theta} ||\\mathbf{A} \\cdot \\boldsymbol \\theta - \\mathbf{y}||^2_2 + \\alpha \\cdot ||\\boldsymbol \\theta||^2_2$$\n",
    "- $-||\\mathbf{A} \\cdot \\boldsymbol \\theta - \\mathbf{y}||^2_2$ is proportional to a Normal log-density\n",
    "- For $-\\alpha \\cdot ||\\boldsymbol \\theta||^2_2$ the same holds: It corresponds to a prior $\\boldsymbol \\theta \\sim \\mathcal{N}(\\mathbf{0}, \\frac{1}{\\alpha} \\cdot \\mathbf{I})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2df627",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to choose a prior?\n",
    "\n",
    "- Unfortunately, it is not always easy\n",
    "- Often problem-dependent &rarr; Investigation and use of existing knowledge necessary\n",
    "- Popular choices can narrow down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8208b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Still, it is always necessary to check your prior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ceafc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Informativeness\n",
    "\n",
    "- Important classification for priors\n",
    "- Generally, informativeness encodes how strong initial assumptions are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffb80e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Assume that $\\boldsymbol\\theta$ is bounded to $\\big[0, 1\\big]$ and expected at $\\frac{1}{2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45d0c8c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pdf(t, samples, ax):\n",
    "    ax.plot(t, samples)\n",
    "    ax.fill_between(t, 0, samples, color=\"#aaaadd\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aae44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey=True, sharex=True, figsize=(15, 5))\n",
    "t = np.linspace(0, 1)\n",
    "plot_pdf(t, scipy.stats.uniform.pdf(t, 0, 1), axs[0])\n",
    "axs[0].set_title(\"uninformative/diffuse\")\n",
    "plot_pdf(t, scipy.stats.truncnorm.pdf((t - 0.5) / 0.5, (0 - 0.5) / 0.5, (1 - 0.5) / 0.5) / 0.5, axs[1])\n",
    "axs[1].set_title(\"weakly informative\")\n",
    "plot_pdf(t, scipy.stats.truncnorm.pdf((t - 0.5) / 0.1, (0 - 0.5) / 0.1, (1 - 0.5) / 0.1) / 0.1, axs[2])\n",
    "axs[2].set_title(\"very informative\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbab3ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shrinkage Priors\n",
    "- Special regularization techniques\n",
    "- If likelihoods indicates close to zero, promote setting zero\n",
    "- Example: spike-and-slab priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387405f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "spike = scipy.stats.norm(loc=0, scale=1e-1)\n",
    "slab = scipy.stats.norm(loc=0, scale=2)\n",
    "t = np.linspace(-5, 5, 10000)\n",
    "plt.plot(t, 0.5 * spike.pdf(t) + 0.5 * slab.pdf(t))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd1f19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conjugate Priors\n",
    "> A conjugate prior is an algebraic convenience, [...]; otherwise, numerical integration may be necessary. - *Wikipedia*\n",
    "- Choose prior such that posterior is from the same distribution family\n",
    "- Posterior distribution can be computed very efficiently!\n",
    "- Works for some combinations of simple likelihoods and priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd8e1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior Predictive Checks\n",
    "\n",
    "- How can we check, if the prior is generally suitable?\n",
    "- Check if prior + model produce reasonable results\n",
    "- Example: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df86aaa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "t = np.linspace(0, 1, num=n)\n",
    "X = np.column_stack((t, np.ones(n)))\n",
    "\n",
    "def show_prior(ax, prior):\n",
    "    theta = prior.rvs(size=1000)\n",
    "    ax.plot(t, X @ theta.T, color=\"red\", alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa29df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5), sharey=True)\n",
    "show_prior(ax[0], scipy.stats.multivariate_normal(np.zeros(2), 1e-1 * np.identity(2)))\n",
    "show_prior(ax[1], scipy.stats.multivariate_normal(np.zeros(2), 10 * np.identity(2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3ffa3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Statistics in Application\n",
    "\n",
    "How can we do inference in general, application-oriented scenarios?\n",
    "1. Circumvent posterior normalization &rarr; Markov Chain Monte Carlo (MCMC)\n",
    "2. Check assumptions &rarr; Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16b29f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov Chain Monte Carlo\n",
    "\n",
    "- Markov chains: special stochastic processes that generates posterior samples\n",
    "- Monte Carlo: Integration by random sampling\n",
    "- Core ideas: Approximate posterior by sampling \n",
    "    - No need for Normalization!\n",
    "    - Use to approximate characteristics like expectation or variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81197de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integration by Monte Carlo\n",
    "- Monte Carlo uses independent samples to compute integrals\n",
    "$$\\mathbb{E}_{\\boldsymbol\\theta \\sim p\\left(\\boldsymbol\\theta \\, | \\, \\mathbf{y}\\right)}\\big(h(\\boldsymbol\\theta)\\big) \\approx \\frac{1}{n} \\sum_{i=1}^{n} h(\\boldsymbol\\theta_i) \\quad \\text{with} \\quad \\boldsymbol\\theta_1, \\dots, \\boldsymbol\\theta_n \\sim p\\left(\\boldsymbol\\theta \\, | \\, \\mathbf{y}\\right) \\, \\text{i.i.d.}$$\n",
    "- Example: $h(\\boldsymbol\\theta) = \\boldsymbol\\theta$ computes the expectation of random variable $\\boldsymbol \\theta$\n",
    "- $\\mathcal{O}\\left(\\sqrt{n}\\right)$ convergence (in $L^2$) is independent of number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69913d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random sampling with Markov Chains\n",
    "\n",
    "<br/>\n",
    "<div style=\"width: 45%; float: left; margin-right: 5%\">\n",
    "    <ul>\n",
    "          <li>Markov chains jump from one state to the next</li>\n",
    "          <li>Jumps are random and only depend on the current state (Markovianity)</li>\n",
    "          <li>For Bayesian statistics: Visited states are samples from posterior</li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div style=\"width: 50%; float: left;\">\n",
    "    <a href=\"https://www.researchgate.net/figure/Illustration-of-the-Markov-chain-Monte-Carlo-with-Metropolis-Hastings-MCMC-MH-procedure_fig6_338363810\"><img src=\"https://www.researchgate.net/publication/338363810/figure/fig6/AS:843249581309953@1578057775228/Illustration-of-the-Markov-chain-Monte-Carlo-with-Metropolis-Hastings-MCMC-MH-procedure.png\" alt=\"Illustration of the Markov chain Monte Carlo with Metropolis-Hastings (MCMC-MH) procedure.\"/></a>\n",
    "    (Tomic et al., 2009)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9951222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sampling with Markov Chains is hard\n",
    "- Samples are correlated!\n",
    "    - Less *effective* samples\n",
    "    - More samples are needed to converge \n",
    "- Difficulties with posterior properties (e.g. multiple modes, non-convexity)\n",
    "- Curse of dimensionality\n",
    "\n",
    "&rarr; MCMC has convergence guarantee, but can be slow in practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d28b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Posterior Predictive Checks\n",
    "\n",
    "- \"A posteriori\" check, if assumptions of Bayesian analysis are correct\n",
    "- How well is the data reproduced? Do error assumptions hold?\n",
    "- Should always be done!\n",
    "- Example: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88fc542",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y = X @ np.array([5., 1.])\n",
    "sigma = 1\n",
    "y_hat = y + np.random.normal(0, sigma, size=n)\n",
    "\n",
    "def posterior_predictive(ax, prior):\n",
    "    cov = np.linalg.inv(X.T @ X / sigma**2 + np.linalg.inv(prior.cov))\n",
    "    mu = cov @ X.T @ y_hat / sigma**2\n",
    "    theta = scipy.stats.multivariate_normal(mu, cov).rvs(size=1000)\n",
    "    ax.plot(t, X @ theta.T, color=\"red\", alpha=0.01)\n",
    "    ax.scatter(t, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bac08f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5), sharex=True, sharey=\"row\")\n",
    "posterior_predictive(ax[0], scipy.stats.multivariate_normal(np.zeros(2), 1e-1 * np.identity(2)))\n",
    "posterior_predictive(ax[1], scipy.stats.multivariate_normal(np.zeros(2), 10 * np.identity(2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ce89f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wrap Up\n",
    "- Bayesian statistics is a flexible framework for inference\n",
    "- Priors require careful investigation, but are very useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55488c20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "What we did not talk about:\n",
    "- Bayesian non-parametrics \n",
    "- Model uncertainty (Bayesian Model Averaging)\n",
    "- Nested data (Hierarchical Bayesian Modeling)\n",
    "- In-depth MCMC\n",
    "- Alternatives to MCMC (e.g. Variational Bayes, Sequential Monte Carlo)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68dc02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Exercises"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
